<!<!DOCTYPE html>
    <html lang="en">

    <head>
        <meta charset="utf-8">
        <title>PRDP (ICLR 2025) - Progressively Refined Differentiable Physics - Project Page</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" href="static/css/academicons.min.css">
        <link rel="stylesheet" href="static/css/fontawesome.min.css">
        <script src="static/js/all.min.js"></script>
        <script src="static/js/bulma-carousel.min.js"></script>


        <link rel="stylesheet" href="static/css/style.css">
        <script src="static/js/script.js"></script>
        <!-- Adding latex equations support -->
        <script>
            MathJax = {
              tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
              svg: { fontCache: 'global' }
            };
        </script>
        <script id="MathJax-script" async 
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>

    <body>
        <nav class="navbar" role="navigation" aria-label="main navigation">
            <div class="navbar-brand">
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                    <a class="navbar-item" href="https://github.com/kanishkbh">
                        <span class="icon">
                            <i class="fas fa-home"></i>
                        </span>
                    </a>

                    <div class="navbar-item has-dropdown is-hoverable">
                        <a class="navbar-link">
                            More Research
                        </a>
                        <div class="navbar-dropdown">
                            <a class="navbar-item" href="https://ge.in.tum.de/publications/">
                                TUM Thuerey Group
                            </a>
                        </div>
                    </div>
                </div>

            </div>
        </nav>


        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h1 class="title is-1 is-size-3-mobile publication-title">PRDP: Progressively Refined Differentiable Physics</h1>
                            <h2 class="subtitle is-4 opacity-1" style="margin-top: 1rem;opacity:0.7">ICLR 2025</h2>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <a href="https://github.com/kanishkbh">Kanishk Bhatia</a>,</span>
                                <span class="author-block">
                                    <a href="https://fkoehler.site/"> Felix Köhler</a>,</span>
                                <span class="author-block">
                                    <a href="https://ge.in.tum.de/about/n-thuerey/">
                                        Nils Thuerey</a>
                                </span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block">Technical University of Munich</span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="https://openreview.net/pdf?id=9Fh0z1JmPU"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://arxiv.org/"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://github.com/tum-pbs/"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <!-- Dataset Link. -->
                                    <!-- <span class="link-block">
                                        <a href="https://huggingface.co/datasets/thuerey-group/apebench-scraped"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="far fa-images"></i>
                                            </span>
                                            <span>Dataset</span>
                                        </a>
                                    </span> -->
                                    <!-- Poster Link. -->
                                    <span class="link-block">
                                        <a href="https://github.com/tum-pbs/apebench-paper/blob/main/apebench_poster.pdf"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-newspaper"></i>
                                            </span>
                                            <span>Poster</span>
                                        </a>
                                    </span>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- <section class="hero teaser is-light">
            <div class="hero-body">
                <div class="container">
                    <div id="teaser">
                        <img src="static/img/teaser_new_transparant.png"/>
                    </div>
                </div>
            </div>
        </section> -->

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- TL;DR. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">What is PRDP?</h2>
                        <div class="content has-text-justified">
                            <b>P</b>rogressively <b>R</b>efined <b>D</b>ifferentiable <b>P</b>hysics (PRDP) is a method to reduce the computational cost of
                            learning pipelines (e.g. training of neural networks) that contain expensive 
                            iterative physics solvers. 
                            <br>
                            It works by letting the learning algorithm use 
                            cheap low-accuracy runs of the physics solver in the beginning, and adaptively refines the physics
                            as the epochs progress. The method also prevents superfluous refinment of the physics solver, 
                            based on our observation that full convergence of the physics is not necessary - 
                            neural emulators, even when trained through incompletely (but sufficiently) converged physics,
                            perform as well as their counterparts trained through fully-converged physics.
                            <br>
                            These two ideas are henceforth referred to as 
                            <b>P</b>rogressive <b>R</b>efinement (PR) and <b>I</b>ncomplete <b>C</b>onvergence (IC).
                            <br>
                            <!-- <img src="static/img/teaser_ns_spatial_sep27_time.pdf" alt="PRDP Teaser" style="width: 100%;"/> -->
                        </div>
                        <div class="column is-centered">
                            <img src="static/img/teaser_ns_spatial_sep27_time.png" alt="PRDP Teaser" style="width: 100%;"/>
                            <p class="caption has-text-centered">Figure 1: <b>PRDP</b> reduces the training time of neural
                                networks containing numerical solver components (c). The fidelity of iterative components is 
                                increased only if validation metrics of the network training plateau. This leads to savings by using fewer iterations in the
                                beginning (<b>PR savings</b> in (b)) and by ending at a refinement level significantly below full fidelity
                                (<b>IC savings</b> in (b)). The achieved validation error is identical (a).</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-5">But what is Differentiable Physics, and why should I care?</h2>
                        <div class="content has-text-justified">
                            Differentiable physics refers to physics solvers written in frameworks that are autodiff friendly,
                            thus automatic differentiation frameworks can pass gradients through these solvers. 
                            This enables the integration of physics-based models in machine learning frameworks, 
                            allowing gradient-based optimization techniques to train models that incorporate physical laws. 
                            <br>
                            Numerous domains have benefitted from differentiable physics, including:
                            <ul>
                                <li>Solving inverse problems (<a href="https://link.springer.com/book/10.1007/978-3-662-05086-6">Bendsoe & Sigmund, 2013)</a></li>
                                <li>Integrating physical constraints (<a href="https://doi.org/10.1016/j.jcp.2018.10.045">Raissi et al., 2019</a>; <a href="https://doi.org/10.1145/3648506">Li et al., 2024</a></li>
                                <li>Creating hybrid models that blend classical numerical techniques with learned components (<a href="https://arxiv.org/abs/2007.00016">Um et al., 2020</a>; <a href="https://arxiv.org/abs/2102.01010">Kochkov et al., 2021</a>; <a href="https://arxiv.org/abs/2311.07222">2024</a>)</li>
                            </ul>
                            <br>
                            However, repeatedly querying a solver with several iterations in
                            the forward pass - and differentiating through its iterations in the backward pass - 
                            can introduces a severe computational bottleneck during training.
                        </div>
                        <div class="column is-centered">
                            <img src="static/img/physics_bottleneck.png" alt="solver-in-the-loop physics bottleneck" style="width: 100%;"/>
                            <p class="caption has-text-centered">Figure 2: 
                                A typical neural correction-learning pipeline (<a href="https://arxiv.org/abs/2007.00016">Um et al., 2020</a>) 
                                that uses a differentiable physics solver $\mathcal{P}_K$ in the loop. 
                                Black arrows show the forward pass, grey dashed arrows represent the backward pass, and elements in red represent 
                                the bottleneck. 
                                As the number of solver iterations $K$ grows, the cost of passes through $\mathcal{P}_K$ becomes severe.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">How does PRDP work?</h2>
                        <div class="content has-text-justified">
                            <p>
                                PRDP's core idea revolves around balancing the compute-accuracy trade-off of iterative physics solvers.
                                The nuancy is - we assert only the accuracy of the neural network being trained, not the physics solver.
                                Hence, the physics solver is not converged to numerical precision, 
                                but is allowed to run only for about as many iterations as sufficient
                                to gain significant training progress in the neural network.
                            </p>
                            <p>
                                <b>How much physics refinement is sufficient?</b>  <br>  
                                We contribute an algorithm that determines physics refinement adaptively 
                                based on the plateauing of training progress measured on a validation metric.
                                Training begins with cheap coarse physics. 
                                When the training progress stagnates, a refinement of the physics is invoked.
                                However, if the progress has stangated over successive physics refinements, indicating
                                that further physics refinement would be superfluous, refinement is stopped.
                            </p>
                            <div class="column is-centered"></div>
                                <img src="static/img/prdp_explainer.png" alt="PRDP Explainer Flowchart" style="width: 100%;"/>
                                <p class="caption has-text-centered">Figure 3: 
                                    Left: the typical training progress of a neural network supported by PRDP. 
                                    Right: a simplified flowchart representation of the PRDP control algorithm.</p>
                            </div>
                            <p>
                                This throttling of the physics solver saves significant compute, especially in cases where 
                                the physics iterations are a compute bottleneck. For instance, in our test problem of training a 
                                neural emulator on a 3D heat equation solver, PRDP saves about 78% in total training time.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">How can I use PRDP?</h2>
                        <div class="content has-text-justified">
                            TODO Write. Insert pseudo code(s) from paper. Ignore following copilot-generated content.
                            <br>
                            Using PRDP involves integrating it into your existing machine learning pipeline that includes a physics solver. Here are the steps to get started:
                            <ol>
                                <li><b>Identify the Physics Solver:</b> Determine the iterative physics solver used in your learning pipeline.</li>
                                <li><b>Implement Progressive Refinement:</b> Modify the solver to start with low-accuracy runs and progressively refine the accuracy as training progresses.</li>
                                <li><b>Monitor Convergence:</b> Implement a mechanism to monitor the convergence of the physics solver and stop refinement when sufficient accuracy is achieved for training.</li>
                                <li><b>Integrate with Training Loop:</b> Ensure that the modified solver is integrated seamlessly with your training loop, allowing gradients to flow through the solver.</li>
                                <li><b>Experiment and Tune:</b> Experiment with different levels of refinement and convergence criteria to find the optimal balance between computational cost and training accuracy.</li>
                            </ol>
                            By following these steps, you can leverage PRDP to reduce the computational cost of training neural networks with physics-based models.
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Abstract. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            The physics solvers employed for neural network training are primarily iterative, 
                            and hence, differentiating through them introduces a severe computational
                            burden as iterations grow large. Inspired by works in bilevel optimization, we
                            show that full accuracy of the network is achievable through physics significantly
                            coarser than fully converged solvers. 
                            We propose <b>P</b>rogressively <b>R</b>efined <b>D</b>ifferentiable <b>P</b>hysics (PRDP), 
                            an approach that identifies the level of physics refinement
                            sufficient for full training accuracy. By beginning with coarse physics, adaptively
                            refining it during training, and stopping refinement at the level adequate for 
                            training, it enables significant compute savings without sacrificing network accuracy.
                            Our focus is on differentiating iterative linear solvers for sparsely discretized 
                            differential operators, which are fundamental to scientific computing. PRDP is 
                            applicable to both unrolled and implicit differentiation. We validate its performance
                            on a variety of learning scenarios involving differentiable physics solvers such as
                            inverse problems, autoregressive neural emulators, and correction-based neural-
                            hybrid solvers. In the challenging example of emulating the Navier-Stokes equations, 
                            we reduce training time by 62%.
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Rollout Performance. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Focus on Rollout Performance</h2>
                        <div class="content has-text-justified">
                            Rather than temporally aggregated metrics, APEBench
                            always returns rollout errors to understand temporal
                            generalization. The evaluation can be done in a wide
                            range of metrics, e.g., classical normalized RMSE
                            metrics, Fourier-based mectrics for certain
                            frequency ranges, and Sobolov-based metrics (H1)
                            that point out mismatches in higher frequencies.
                        </div>
                        <div class="content has-text-justified">
                            <!-- <img src="static/img/ks_test_rollout.webm" style="width: 100%;"/> -->
                            <video  loop autoplay muted style="width: 100%;">
                                <source src="static/img/ks_test_rollout.webm" type="video/webm"/>
                                <!-- <img src="static/img/ks_test_rollout.gif" style="width: 100%;"/> -->
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Difficulties. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Unified PDE identifiers</h2>
                        <div class="content has-text-justified">
                            Describing Dynamics with a reduced set of
                            information creates an exchange protocol that
                            uniquely identifies an experiment. These
                            "difficulties" also encode the challenge of neural
                            emulation by including spatial resolution and
                            spatial dimensions.
                        </div>
                        <div class="content has-text-justified">
                            For example, in the animation below over
                            diffusivity, convectivity, and dispersivity we can
                            describe a wide range of PDEs: Diffusion, Burgers,
                            Korteweg-de Vries, Dispersion, and
                            Dispersion-Diffusion.
                        </div>
                        <div class="content has-text-justified">
                            <video  loop autoplay muted style="width: 100%;">
                                <source src="static/img/difficulty_animation.webm" type="video/webm"/>
                                <!-- <img src="static/img/difficulty_animation.gif" style="width: 100%;"/> -->
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Unrolled Training. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Built-In Support for Unrolled Training</h2>
                        <div class="content has-text-justified">
                            APEBench is built around autoregressive emulation
                            and hence emphasizes the temporal axis in emulator
                            learning. This includes the option for unrolled
                            training (also called
                            autoregressive/recursive/rollout training). We unify
                            many approaches in terms of main chain (=unrolled)
                            length T and branch chain (=reference) length B.
                        </div>
                        <div class="column is-centered">
                            <img src="static/img/unrolled_objective.png" style="width: 100%;"/>
                        </div>
                        <div class="content has-text-justified">
                            One-Step supervised training is T=B=1, while
                            five-step unrolled training is T=B=5. Branch-one
                            diverted chain training is T=5, B=1. The latter requires
                            a differentiable solver, readily available in APEBench.
                        <div class="content has-text-justified">
                            <video  loop autoplay muted style="width: 100%;">
                                <source src="static/img/burgers_training_animation.webm" type="video/webm"/>
                                <!-- <img src="static/img/burgers_training_animation.gif" style="width: 100%;"/> -->
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Wide Range of Dynamics. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">A wide range of Dynamics</h2>
                        <div class="content has-text-justified">
                        Accessible via the reduced difficulty or normalized
                        interfaces or via a physical interface. Most dynamics
                        are available in 1D, 2D, and 3D.
                        </div>
                        <div class="content has-text-justified">
                            <table class="table is-striped is-hoverable is-fullwidth is-narrow is-centered" style="text-align: center; font-size: xx-small;">
                                <tr>
                                    <th>Difficulty</th>
                                    <th>Phsical</th>
                                    <th>Normalized</th>
                                </tr>
                                <tr><td>diff_lin            </td><td>phy_poisson         </td><td>norm_lin            </td></tr>
                                <tr><td>diff_lin_simple     </td><td>phy_sh              </td><td>norm_adv            </td></tr>
                                <tr><td>diff_adv            </td><td>phy_gs              </td><td>norm_diff           </td></tr>
                                <tr><td>diff_diff           </td><td>phy_gs_type         </td><td>norm_adv_diff       </td></tr>
                                <tr><td>diff_adv_diff       </td><td>phy_decay_turb      </td><td>norm_disp           </td></tr>
                                <tr><td>diff_disp           </td><td>phy_kolm_flow       </td><td>norm_fisher         </td></tr>
                                <tr><td>diff_hyp_diff       </td><td>phy_lin             </td><td>norm_four           </td></tr>
                                <tr><td>diff_four           </td><td>phy_lin_simple      </td><td>norm_hypdiff        </td></tr>
                                <tr><td>diff_conv           </td><td>phy_adv             </td><td>norm_nonlin         </td></tr>
                                <tr><td>diff_burgers        </td><td>phy_diff            </td><td>norm_conv           </td></tr>
                                <tr><td>diff_kdv            </td><td>phy_adv_diff        </td><td>norm_burgers        </td></tr>
                                <tr><td>diff_ks_cons        </td><td>phy_disp            </td><td>norm_kdv            </td></tr>
                                <tr><td>diff_ks             </td><td>phy_hyp_diff        </td><td>norm_ks_cons        </td></tr>
                                <tr><td>diff_nonlin         </td><td>phy_four            </td><td>norm_ks             </td></tr>
                                <tr><td>diff_burgers_sc     </td><td>phy_nonlin          </td><td>norm_burgers_sc     </td></tr>
                                <tr><td>diff_fisher         </td><td>phy_burgers_sc      </td><td>norm_lin_simple     </td></tr>
                                <tr><td>                    </td><td>phy_kdv             </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_ks              </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_conv            </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_burgers         </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_ks_cons         </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_poly            </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_fisher          </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_unbal_adv       </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_diag_diff       </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_aniso_diff      </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_mix_disp        </td><td>                    </td></tr>
                                <tr><td>                    </td><td>phy_mix_hyp         </td><td>                    </td></tr>
                            </table> 
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Procedural Data Generation. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Procedural data generation</h2>
                        <div class="content has-text-justified">
                            APEBench's embedded pseudo-spectral solver is very
                            efficient. All training and test data is
                            procedurally (deterministically) generated
                            on-the-fly. There is no need to download large
                            datasets, and the simulator can be embedded into the
                            training loop for all kinds of differentiable
                            physics like "solver-in-the-loop" correction setups.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/apebench_slide.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- APEBench Workflow. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Simplified Study Workflow</h2>
                        <div class="content has-text-justified">
                            An APEBench study is a list of dictionaries which
                            APEBench executes and conviently returns Pandas
                            dataframes that can be used for statistical
                            postprocessing (e.g., over random seeds) via Seaborn.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/apebench_workflow.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Seeds first. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Seed Statistics are a first-class Citizen</h2>
                        <div class="content has-text-justified">
                            APEBench inherently supports re-running experiments
                            with different random seeds (for network
                            initialization, stochastic minibatching and
                            optionally also for the procedural data generation).
                            Seed statistics allow for clearly determining a
                            superior emulator architecture or learning
                            methodology based on hypothesis testing. For 1D
                            scenarios, APEBench can parallelize multiple seeds
                            on one GPU to obtain seed statistics virtually for
                            free.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/apebench_seed_workflow.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Integrated Volume Render. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Integrated Volume Renderer</h2>
                        <div class="content has-text-justified">
                            APEBench is accompanied by an efficient
                            (Rust/WebGPU-based) volume renderer to quickly
                            visualize 2D and 3D trajectories. <a
                            href="https://keksboter.github.io/vape4d/">Try it
                            yourself</a> based on a five-axis NumPy array (time
                            x channel x space_0 x space_1 x space_2) or <a
                            href="https://keksboter.github.io/vape4d/?file=https://huggingface.co/datasets/vollautomat/vape4d/resolve/main/gray_scott_3d.npy&colormap=https://huggingface.co/datasets/vollautomat/vape4d/resolve/main/colormap.json">with
                            precomputed Gray-Scott data</a> (Caution! This
                            downloads ~100MB for the opened tab).
                        </div>
                        <div class="content has-text-justified">
                            <video  loop autoplay muted style="width: 100%;">
                                <source src="static/img/volume_render_animation.webm" type="video/webm"/>
                                <!-- <img src="static/img/volume_render_animation.gif" style="width: 100%;"/> -->
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Relating Emulators with Classical Simulators. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">The Relation between Neural Emulators and Numerical Simulators</h2>
                        <div class="content has-text-justified">
                            The fine-grained control over the emulation
                            scenarios allows for drawing analogies between
                            neural emulation and classical numerical simulation.
                            For example, (a) the performance of
                            convolution-based architectures is bound by their
                            receptive field and the difficulty (γ₁ = CFL) of the
                            advection scenario. On the other hand, (a) the
                            pseudo-spectral FNO architecture is agnostic to
                            changes in γ₁. (b) For the highest difficulty,
                            unrolling improves the accuracy of the ResNet.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/adv_experiment_together.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Neural-Hybrid Emulators. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Benchmark Neural-Hybrid Emulators with Differentiable Physics</h2>
                        <div class="content has-text-justified">
                            With the embedded differentiable solver, APEBench
                            can investigate neural-hybrid correction setups. For
                            example, if both ResNet and FNO are used either as
                            full prediction emulators or neural-hybrid emulators
                            for 2D advection (γ₁=10.5) with a coarse solver
                            doing 10% or 50% of the difficulty. Training with
                            unrolling benefits the limited receptive field
                            ResNet yet only shows marginal improvement for the
                            FNO. The ResNet can work in symbiosis with a coarse
                            simulator.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/correction_results.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Comparison across Architectures and PDE Dynamics. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">A wide range of architectures and PDE dynamics</h2>
                        <div class="content has-text-justified">
                            The wide range of PDE dynamics in 1D, 2D, and 3D
                            allows for drawing further analogies. In the paper,
                            we investigated a subset of the 46 PDE dynamics.
                            <!-- The ResNet consistently performs well across
                            all dynamics and dimensions. Local architectures
                            struggle with higher-order derivatives, while
                            limited active modes hinder the FNO's performance in
                            some cases. The Dilated ResNet is the better
                            long-range architecture in 1D, whereas the UNet is
                            better suited for higher dimensions. -->
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/broad_comparison.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Architecture Decision Tree. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">APEBench's experiments suggest neural architectures</h2>
                        <div class="content has-text-justified">
                            Ultimately, with the studies conducted for the
                            APEBench paper, we can suggest emulator
                            architectures with the following decision tree.
                        </div>
                        <div class="content has-text-justified">
                            <img src="static/img/architecture_decision_tree.png" style="width: 100%;"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        


        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <h2 class="title">BibTeX</h2>
                <pre><code>
@article{bhatia2024prdp,
    title={Progressively Refined Differentiable Physics},
    author={Kanishk Bhatia and Felix Koehler and Nils Thuerey},
    journal={International Conference on Learning Representations (ICLR)},
    volume={13},
    year={2025}
}
                </code></pre>
            </div>
        </section>


        <footer class="footer">
            <div class="container">
                <div class="content has-text-centered">
                    <a class="icon-link" href="https://arxiv.org">
                        <i class="fas fa-file-pdf"></i>
                    </a>
                    <a class="icon-link" href="https://github.com/kanishkbh" class="external-link" disabled>
                        <i class="fab fa-github"></i>
                    </a>
                </div>
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <p>
                                Website source code borrowed from <a href="https://keunhong.com">Keunhong Park</a>'s <a
                                    href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </body>

    </html>
